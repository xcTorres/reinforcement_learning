# Reinforcement_learning
The demo code to study reinforcement learning. 

---
## å…«å¤§è¦ç´    
ç¯å¢ƒçŠ¶æ€S  
ä¸ªä½“åŠ¨ä½œA  
ç¯å¢ƒå¥–åŠ±R  
ä¸ªä½“policy  
é‡‡å–è¡ŒåŠ¨ä¹‹åçš„ä»·å€¼value  
rå¥–åŠ±è¡°å‡å› å­  
çŠ¶æ€è½¬åŒ–æ¨¡å‹  
æ¢ç´¢ç‡  

---
## é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹
$v_{\pi}(s) = \sum\limits_{a \in A} \pi(a|s)(R_s^a + \gamma \sum\limits_{s' \in S}P_{ss'}^av_{\pi}(s')) \\
q_{\pi}(s,a) = R_s^a + \gamma \sum\limits_{s' \in S}P_{ss'}^a\sum\limits_{a' \in A} \pi(a'|s')q_{\pi}(s',a')
$

---

### æœ€ä¼˜ä»·å€¼å‡½æ•°æ¨ç†  
$$
v_{*}(s) = \max_{\pi}v_{\pi}(s) \\ 
q_{*}(s,a) = \max_{\pi}q_{\pi}(s,a) \\
\pi_{*}(a|s)= \begin{cases} 1 & {if\;a=\arg\max_{a \in A}q_{*}(s,a)}\\ 0 & {else} \end{cases} \\
v_{*}(s) = \max_{a}q_{*}(s,a) \\
q_{*}(s,a) = R_s^a + \gamma \sum\limits_{s' \in S}P_{ss'}^av_{*}(s') \\  
v_{*}(s) = \max_a(R_s^a + \gamma \sum\limits_{s' \in S}P_{ss'}^av_{*}(s')) \\
q_{*}(s,a) = R_s^a + \gamma \sum\limits_{s' \in S}P_{ss'}^a\max_{a'}q_{*}(s',a')
$$

---
## Dynamic Programming  


---
## Monte Carlo  

$$
\mu_k = \frac{1}{k}\sum\limits_{j=1}^k x_j = \frac{1}{k}(x_k + \sum\limits_{j=1}^{k-1}x_j) =  \frac{1}{k}(x_k + (k-1)\mu_{k-1}) = \mu_{k-1} +  \frac{1}{k}(x_k -\mu_{k-1}) \\ 
N(S_t) = N(S_t)  +1 \\
V(S_t) = V(S_t)  + \frac{1}{N(S_t)}(G_t -  V(S_t) ) \\
$$

---
### TD   

$$
G(t) = R_{t+1} + \gamma V(S_{t+1}) \\
V(S_t) = V(S_t)  + \alpha(G_t -  V(S_t) ) \\
Q(S_t, A_t) = Q(S_t, A_t) + \alpha(G_t -  Q(S_t, A_t))
$$


---
### SARSA  
$$
Q(S,A) = Q(S,A) + \alpha(R+\gamma Q(S',A') - Q(S,A))
$$

---
### Q learning  

$$
Q(S,A) = Q(S,A) + \alpha(R+\gamma \max_aQ(S',a) - Q(S,A))
$$

---
### DQN 

$$
y_j= \begin{cases} R_j& {is\_end_j\; is \;true}\\ R_j + \gamma\max_{a'}Q(\phi(S'_j),A'_j,w) & {is\_end_j \;is\; false} \end{cases}
$$

---
### Nature DQN  
ã€€ã€€è¿™é‡Œç›®æ ‡Qå€¼çš„è®¡ç®—ä½¿ç”¨åˆ°äº†å½“å‰è¦è®­ç»ƒçš„Qç½‘ç»œå‚æ•°æ¥è®¡ç®—ğ‘„(ğœ™(ğ‘†â€²ğ‘—),ğ´â€²ğ‘—,ğ‘¤)ï¼Œè€Œå®é™…ä¸Šï¼Œæˆ‘ä»¬åˆå¸Œæœ›é€šè¿‡ğ‘¦ğ‘—æ¥åç»­æ›´æ–°Qç½‘ç»œå‚æ•°ã€‚è¿™æ ·ä¸¤è€…å¾ªç¯ä¾èµ–ï¼Œè¿­ä»£èµ·æ¥ä¸¤è€…çš„ç›¸å…³æ€§å°±å¤ªå¼ºäº†ã€‚ä¸åˆ©äºç®—æ³•çš„æ”¶æ•›ã€‚å› æ­¤ï¼Œä¸€ä¸ªæ”¹è¿›ç‰ˆçš„DQN: Nature DQNå°è¯•ç”¨ä¸¤ä¸ªQç½‘ç»œæ¥å‡å°‘ç›®æ ‡Qå€¼è®¡ç®—å’Œè¦æ›´æ–°Qç½‘ç»œå‚æ•°ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚

$$
y_j= \begin{cases} R_j& {is\_end_j\; is \;true}\\ R_j + \gamma\max_{a'}Q'(\phi(S'_j),A'_j,w') & {is\_end_j \;is\; false} \end{cases}
$$

---
### Double DQN  

$$
y_j= \begin{cases} R_j& {is\_end_j\; is \;true}\\ R_j + \gamma Q'(\phi(S'_j),\arg\max_{a'}Q(\phi(S'_j),a,w),w')& {is\_end_j\; is \;false} \end{cases}
$$

---
### Prioritized Replay DQN


---
### Dueling DQN  

---
### DDPG
 
